{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wat is een Recurrent Neural Network?\n",
    "\n",
    "Een Recurrent Neural Network (RNN) is een type kunstmatig neuraal netwerk dat speciaal is ontworpen voor sequentiële data of data die een tijdsdimensie heeft. Dit maakt RNN's zeer geschikt voor toepassingen zoals spraakherkenning, taalmodellering, vertaling, en tijdreeksvoorspelling.\n",
    "\n",
    "#### Hoe werkt een Recurrent Neural Network?\n",
    "\n",
    "##### Basisconcepten:\n",
    "\n",
    "1. Tijdsdimensie en Sequentiële Data: In een RNN wordt invoer opgedeeld in tijdstappen, waarbij elke stap overeenkomt met een positie in de sequentie. Bijvoorbeeld, een zin kan worden opgedeeld in woorden, waarbij elk woord een tijdstap vertegenwoordigt.\n",
    "\n",
    "2. Hidden States (Verborgen Staten): Een RNN heeft een verborgen toestand die informatie van de vorige tijdstappen bevat. Deze toestand wordt doorgegeven van de ene tijdstap naar de volgende en wordt continu bijgewerkt.\n",
    "\n",
    "3. Netwerkstructuur:\n",
    "    - Invoerlaag: Ontvangt de huidige invoer $x_t$.\n",
    "    - Verborgen laag: Verwerkt de huidige invoer samen met de vorige verborgen toestand $h_{t-1}$ om de nieuwe verborgen toestand $h_t$ te berekenen.\n",
    "    - Uitvoerlaag: Produceert de uiteindelijke uitvoer $y_t$ op basis van de verborgen toestand $h_t$.\n",
    "\n",
    "##### Wiskundige Representatie:\n",
    "\n",
    "Bij elke tijdstap $t$ worden de volgende berekeningen uitgevoerd:\n",
    "\n",
    "- Invoer naar verborgen laag:\n",
    "$$ h_t = tanh(W_{hx}x_t + W_{hh}h_t + b_h) $$\n",
    "\n",
    "Hierin is $W_{hx}$ de gewichtsparameter tussen de invoer en de verborgen laag, $W_{hh}$ de gewichtsparameter tussen de vorige verborgen toestand en de huidige verborgen toestand, en $b_h$ de bias.\n",
    "\n",
    "- Verborgen laag naar uitvoer:\n",
    "$$ y_t = W_{hy}h_t + b_y $$\n",
    "\n",
    "Hierin is $W_{hy}$ de gewichtsparameter tussen de verborgen laag en de uitvoerlaag, en $b_y$ de bias.\n",
    "\n",
    "##### Trainen van een RNN:\n",
    "\n",
    "Het trainen van een RNN gebeurt door middel van backpropagation through time (BPTT), een uitbreiding van het standaard backpropagation-algoritme. BPTT werkt door de fouten van de uitvoer terug te propagateren door de tijdstappen van de sequentie, waarbij de gewichten worden bijgewerkt om de fout te minimaliseren.\n",
    "Waarom RNN's nuttig zijn\n",
    "\n",
    "1. Geheugen van Tijdelijke Patronen: RNN's kunnen eerdere informatie onthouden, wat cruciaal is voor taken zoals taalmodellering, waarbij de betekenis van een woord vaak afhangt van eerdere woorden in de zin.\n",
    "\n",
    "2. Flexibiliteit: Ze kunnen worden toegepast op variabele lengte sequenties, wat betekent dat ze geschikt zijn voor verschillende soorten data zoals tekst, audio en tijdreeksen.\n",
    "\n",
    "3. Dynamische Behandeling van Sequenties: In tegenstelling tot traditionele neurale netwerken die statische input vereisen, kunnen RNN's dynamische sequenties van gegevens verwerken, wat ze bijzonder geschikt maakt voor real-time toepassingen.\n",
    "\n",
    "##### Uitdagingen en Oplossingen\n",
    "\n",
    "**Vanishing and Exploding Gradients**\n",
    "\n",
    "Een veel voorkomend probleem bij RNN's is dat tijdens het trainen de gradiënten kunnen verdwijnen (ze worden extreem klein) of exploderen (ze worden extreem groot). Dit maakt het moeilijk om lange termijn afhankelijkheden te leren.\n",
    "\n",
    "**Oplossingen:**\n",
    "\n",
    "- Long Short-Term Memory (LSTM): Dit is een speciale vorm van RNN die gebruik maakt van cellen die expliciet lange termijn geheugen kunnen bewaren en bijwerken door middel van een complexe structuur van poorten (ingangs-, uitgangs- en vergeetpoorten). Dit helpt bij het behouden van relevante informatie over lange tijdsperioden.\n",
    "Gated Recurrent Units (GRU): Een vereenvoudigde versie van LSTM's die ook gebruik maakt van poorten om informatie te behouden en te vergeten, maar met minder parameters en een eenvoudiger structuur.\n",
    "\n",
    "##### Toepassingen van RNN's\n",
    "\n",
    "- Taalmodellering en Tekstgeneratie: Het voorspellen van het volgende woord in een zin.\n",
    "- Spraakherkenning: Omzetten van spraak naar tekst door de opeenvolging van geluiden te interpreteren.\n",
    "- Vertaling: Automatische vertaling van tekst van de ene taal naar de andere.\n",
    "- Tijdreeksvoorspelling: Voorspellen van toekomstige waarden op basis van historische gegevens, zoals aandelenkoersen.\n",
    "\n",
    "Kortom, Recurrent Neural Networks zijn krachtige hulpmiddelen voor het werken met sequentiële gegevens, dankzij hun vermogen om contextuele informatie door de tijd heen te onthouden en te verwerken. Dit maakt ze essentieel voor een breed scala aan toepassingen in de moderne kunstmatige intelligentie."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
