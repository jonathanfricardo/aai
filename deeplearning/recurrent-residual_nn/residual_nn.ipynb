{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Een Recurrent Neural Network (RNN) is een type kunstmatig neuraal netwerk dat speciaal is ontworpen voor sequentiële data of tijdreeksdata. Dit maakt RNN's bijzonder geschikt voor toepassingen zoals natuurlijke taalverwerking (NLP), spraakherkenning, vertalingen, en andere domeinen waar de volgorde en context van de data van groot belang zijn.\n",
    "\n",
    "##### Basisprincipes van Recurrent Neural Networks\n",
    "\n",
    "In traditionele feedforward neurale netwerken stroomt de informatie slechts in één richting: van de inputlaag, door de verborgen lagen, naar de outputlaag. Dit maakt ze beperkt in het verwerken van sequentiële data omdat ze geen geheugen hebben voor eerder verwerkte informatie.\n",
    "\n",
    "RNN's introduceren echter terugkoppellussen waardoor ze in staat zijn informatie van eerdere stappen in de sequentie te onthouden en te gebruiken bij de verwerking van nieuwe input. Dit gebeurt door een verborgen toestand (hidden state) die wordt bijgewerkt bij elke tijdstap en de informatie van de vorige tijdstappen bevat.\n",
    "\n",
    "##### Werking van een RNN\n",
    "\n",
    "1. Input en Hidden State:\n",
    "    - Op elk tijdstap $t$ wordt een input $x_t$​ gegeven.\n",
    "    - De hidden state $h_t$​ wordt berekend op basis van de huidige input $x_t$​ en de vorige hidden state $h_{t-1}$​.\n",
    "\n",
    "2. Berekening:\n",
    "    - De hidden state wordt berekend met de formule:\n",
    "    $$ h_t = σ(W_hW_{h-t} + W_xx_t + b) $$\n",
    "    \n",
    "    - Hierin zijn $W_h$​ en $W_x$​ gewichten, $b$ is een bias, en $σ$ is een activatiefunctie (zoals tanh of ReLU).\n",
    "\n",
    "3. Output:\n",
    "    De output $y_t$​ op tijdstap $t$ kan worden berekend met:\n",
    "    $$ y_t = softmax(W_yh_t + c) $$\n",
    "    waarbij $W_y$​ de gewichten zijn die de hidden state naar de output map, en $c$ een bias is.\n",
    "\n",
    "##### Voordelen van RNN's\n",
    "\n",
    "- Contextafhankelijkheid: RNN's kunnen contextuele informatie over de sequentie behouden en gebruiken, wat cruciaal is voor taken zoals taalmodellen en sequentievoorspelling.\n",
    "- Dynamisch inputlengte: RNN's kunnen sequenties van variabele lengte verwerken, wat handig is voor real-world data die niet altijd dezelfde lengte heeft.\n",
    "\n",
    "##### Uitdagingen en Verbeteringen\n",
    "\n",
    "1. Vanishing en Exploding Gradients:\n",
    "    Een bekende uitdaging bij RNN's is het probleem van verdwijnende of exploderende gradiënten tijdens het trainen, vooral bij lange sequenties. Dit betekent dat de gradienten tijdens backpropagation zeer klein of zeer groot kunnen worden, wat het leren bemoeilijkt.\n",
    "2. LSTM en GRU:\n",
    "    Om deze problemen aan te pakken, zijn geavanceerdere varianten van RNN's ontwikkeld, zoals Long Short-Term Memory (LSTM) en Gated Recurrent Units (GRU). Deze modellen bevatten mechanismen (zoals gates) die belangrijkere informatie kunnen onthouden en minder belangrijke informatie kunnen vergeten, waardoor ze beter in staat zijn om lange-termijn afhankelijkheden te leren.\n",
    "\n",
    "##### Toepassingen\n",
    "\n",
    "- Natuurlijke taalverwerking: Voor taken zoals taalmodellering, tekstgeneratie, en machinevertaling.\n",
    "- Spraakherkenning: Voor het omzetten van spraak naar tekst.\n",
    "- Tijdreeksvoorspelling: Voor financiële voorspellingen, weersvoorspellingen, en andere toepassingen waar gegevens over de tijd heen worden geanalyseerd.\n",
    "- Beeld- en video-analyse: Voor sequentiële beeldverwerking en video-analyse.\n",
    "\n",
    "##### Samenvatting\n",
    "\n",
    "RNN's zijn krachtig voor het modelleren van sequentiële data vanwege hun vermogen om informatie van eerdere tijdstappen te onthouden en te gebruiken. Hoewel ze enkele uitdagingen hebben, zoals het probleem van verdwijnende en exploderende gradienten, hebben verbeteringen zoals LSTM en GRU deze problemen deels opgelost. Hun vermogen om context en volgorde te begrijpen maakt ze essentieel in vele moderne AI-toepassingen."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
