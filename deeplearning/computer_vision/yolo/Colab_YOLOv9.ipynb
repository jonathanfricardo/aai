{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfvmV_-ock1V"
      },
      "source": [
        "# Google Colab Notebook YOLOv9\n",
        "\n",
        "Implementation of paper - [YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information](https://arxiv.org/abs/2402.13616)\n",
        "\n",
        "This Colab Notebook is a demo version of the [Yolov9 model](https://github.com/WongKinYiu/yolov9).\n",
        "\n",
        "<div align=\"center\">\n",
        "    <a href=\"./\">\n",
        "        <img src=\"https://huggingface.co/adonaivera/yolov9/resolve/main/performance.png\" width=\"79%\"/>\n",
        "    </a>\n",
        "</div>\n",
        "\n",
        "\n",
        "## Performance\n",
        "\n",
        "MS COCO\n",
        "\n",
        "| Model | Test Size | AP<sup>val</sup> | AP<sub>50</sub><sup>val</sup> | AP<sub>75</sub><sup>val</sup> | Param. | FLOPs |\n",
        "| :-- | :-: | :-: | :-: | :-: | :-: | :-: |\n",
        "| [**YOLOv9-S**]() | 640 | **46.8%** | **63.4%** | **50.7%** | **7.2M** | **26.7G** |\n",
        "| [**YOLOv9-M**]() | 640 | **51.4%** | **68.1%** | **56.1%** | **20.1M** | **76.8G** |\n",
        "| [**YOLOv9-C**](https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-c.pt) | 640 | **53.0%** | **70.2%** | **57.8%** | **25.5M** | **102.8G** |\n",
        "| [**YOLOv9-E**](https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-e.pt) | 640 | **55.6%** | **72.8%** | **60.6%** | **58.1M** | **192.5G** |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgXeBsdCdm_A"
      },
      "source": [
        "## Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlsGVhscqjY0",
        "outputId": "d1fc238d-add2-4107-d34e-2164436d41e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov9'...\n",
            "remote: Enumerating objects: 627, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 627 (delta 2), reused 1 (delta 0), pack-reused 621\u001b[K\n",
            "Receiving objects: 100% (627/627), 3.21 MiB | 9.80 MiB/s, done.\n",
            "Resolving deltas: 100% (241/241), done.\n",
            "/Users/jonathanricardo/Library/CloudStorage/OneDrive-HvA/Semester 4 - AI/Logboek AAI/minor-logboek-aai-2023-jonathan/deeplearning/computer_vision/yolo/yolov9/yolov9\n"
          ]
        }
      ],
      "source": [
        "!git clone --recursive https://github.com/WongKinYiu/yolov9.git\n",
        "%cd yolov9/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWLbrNBZ-RZ4",
        "outputId": "fbe3d0f7-27db-4691-e83f-edddd25f1baa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/jonathanricardo/Library/CloudStorage/OneDrive-HvA/Semester 4 - AI/Logboek AAI/minor-logboek-aai-2023-jonathan/deeplearning/computer_vision/yolo/yolov9/yolov9\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uE1GmCSAJHXC",
        "outputId": "4b053030-ef34-4340-cbf6-f2b8d2cab6f1"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt -q\n",
        "!pip install supervision -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siKauuGOjtC_"
      },
      "source": [
        "## Load example Images and Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lIGzbdh7i7VF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: wget\n"
          ]
        }
      ],
      "source": [
        "!wget -P {HOME} -q https://storage.googleapis.com/adonaivera_certified/test.jpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hA2bGN2bxhRl"
      },
      "outputs": [],
      "source": [
        "!mkdir -p {HOME}/weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zQ17JmPgxkFt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: wget\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: wget\n"
          ]
        }
      ],
      "source": [
        "!wget -P {HOME}/weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-c.pt\n",
        "!wget -P {HOME}/weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-e.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO8LQsf5yH_p"
      },
      "source": [
        "## Inference with pre-trained models in Images with YoloV9 and Supervision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "k-JxtpQ_2e0F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from models.common import DetectMultiBackend\n",
        "from utils.general import non_max_suppression, scale_boxes\n",
        "from utils.torch_utils import select_device, smart_inference_mode\n",
        "from utils.augmentations import letterbox\n",
        "import PIL.Image\n",
        "import supervision as sv\n",
        "\n",
        "@smart_inference_mode()\n",
        "def predict(image_path, weights='yolov9-c.pt', imgsz=640, conf_thres=0.1, iou_thres=0.45, device='0', data='data/coco.yaml'):\n",
        "    # Initialize\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weights, device=device, fp16=False, data=data)\n",
        "    stride, names, pt = model.stride, model.names, model.pt\n",
        "\n",
        "    # Load image\n",
        "    image = PIL.Image.open(image_path)\n",
        "    img0 = np.array(image)\n",
        "    assert img0 is not None, f'Image Not Found {image_path}'\n",
        "    img = letterbox(img0, imgsz, stride=stride, auto=True)[0]\n",
        "    img = img[:, :, ::-1].transpose(2, 0, 1)\n",
        "    img = np.ascontiguousarray(img)\n",
        "    img = torch.from_numpy(img).to(device).float()\n",
        "    img /= 255.0\n",
        "    if img.ndimension() == 3:\n",
        "        img = img.unsqueeze(0)\n",
        "\n",
        "    # Init bounding box annotator and label annotator\n",
        "    bounding_box_annotator = sv.BoxAnnotator()\n",
        "    label_annotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n",
        "\n",
        "    # Inference\n",
        "    pred = model(img, augment=False, visualize=False)\n",
        "\n",
        "    # Apply NMS\n",
        "    pred = non_max_suppression(pred[0][0], conf_thres, iou_thres, classes=None, max_det=1000)\n",
        "\n",
        "    # Process detections\n",
        "    for i, det in enumerate(pred):\n",
        "        if len(det):\n",
        "            det[:, :4] = scale_boxes(img.shape[2:], det[:, :4], img0.shape).round()\n",
        "            for *xyxy, conf, cls in reversed(det):\n",
        "                label = f'{names[int(cls)]} {conf:.2f}'\n",
        "                # Transform detections to supervisions detections\n",
        "                detections = sv.Detections(\n",
        "                    xyxy=torch.stack(xyxy).cpu().numpy().reshape(1, -1),\n",
        "                    class_id=np.array([int(cls)]),\n",
        "                    confidence=np.array([float(conf)])\n",
        "                )\n",
        "\n",
        "                # Labels\n",
        "                labels = [\n",
        "                    f\"{class_id} {confidence:0.2f}\"\n",
        "                    for class_id, confidence\n",
        "                    in zip(detections.class_id, detections.confidence)\n",
        "                ]\n",
        "\n",
        "                img0 = bounding_box_annotator.annotate(img0, detections)\n",
        "                img0 = label_annotator.annotate(img0, detections, labels)\n",
        "\n",
        "    return img0[:, :, ::-1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWlaz2RQ_VqY"
      },
      "source": [
        "If you want to test your own image or try another YoloV9 model, modify the image_path or the weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Bjp3Me-eBQno"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "u9ZdA2xx27nl",
        "outputId": "cf88f7ec-95b5-4ae5-9afc-592093d4ae56"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "git: '4' is not a git command. See 'git --help'.\n",
            "\n",
            "The most similar command is\n",
            "\tp4\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "Invalid CUDA '--device 0' requested, use '--device cpu' or pass valid CUDA device(s)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m img \u001b[38;5;241m=\u001b[39m predict(image_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHOME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/test.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m,weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHOME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/weights/yolov9-e.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m sv\u001b[38;5;241m.\u001b[39mplot_image(img)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "Cell \u001b[0;32mIn[16], line 14\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(image_path, weights, imgsz, conf_thres, iou_thres, device, data)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;129m@smart_inference_mode\u001b[39m()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(image_path, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov9-c.pt\u001b[39m\u001b[38;5;124m'\u001b[39m, imgsz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m640\u001b[39m, conf_thres\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, iou_thres\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.45\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/coco.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Initialize\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     device \u001b[38;5;241m=\u001b[39m select_device(device)\n\u001b[1;32m     15\u001b[0m     model \u001b[38;5;241m=\u001b[39m DetectMultiBackend(weights, device\u001b[38;5;241m=\u001b[39mdevice, fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m     16\u001b[0m     stride, names, pt \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstride, model\u001b[38;5;241m.\u001b[39mnames, model\u001b[38;5;241m.\u001b[39mpt\n",
            "File \u001b[0;32m~/Library/CloudStorage/OneDrive-HvA/Semester 4 - AI/Logboek AAI/minor-logboek-aai-2023-jonathan/deeplearning/computer_vision/yolo/yolov9/yolov9/utils/torch_utils.py:114\u001b[0m, in \u001b[0;36mselect_device\u001b[0;34m(device, batch_size, newline)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device:  \u001b[38;5;66;03m# non-cpu device requested\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m device  \u001b[38;5;66;03m# set environment variable - must be before assert is_available()\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(device\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)), \\\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--device \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m requested, use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--device cpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or pass valid CUDA device(s)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cpu \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mps \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():  \u001b[38;5;66;03m# prefer GPU if available\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     devices \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# range(torch.cuda.device_count())  # i.e. 0,1,6,7\u001b[39;00m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Invalid CUDA '--device 0' requested, use '--device cpu' or pass valid CUDA device(s)"
          ]
        }
      ],
      "source": [
        "img = predict(image_path=f'{HOME}/test.jpeg',weights=f'{HOME}/weights/yolov9-e.pt')\n",
        "sv.plot_image(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFWqhKynxse6"
      },
      "source": [
        "\n",
        "## Citation\n",
        "\n",
        "```\n",
        "@article{wang2024yolov9,\n",
        "  title={{YOLOv9}: Learning What You Want to Learn Using Programmable Gradient Information},\n",
        "  author={Wang, Chien-Yao  and Liao, Hong-Yuan Mark},\n",
        "  booktitle={arXiv preprint arXiv:2402.13616},\n",
        "  year={2024}\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2hCsemH735b"
      },
      "source": [
        "## Contributor Google Colab\n",
        "\n",
        "Author Yolo V9: Wang, Chien-Yao  and Liao, Hong-Yuan Mark. [YoloV9 GitHub](https://github.com/WongKinYiu/yolov9) ðŸš€\n",
        "\n",
        "\n",
        "Who created the Google Colab demo: [AdonaiVera's GitHub](https://github.com/AdonaiVera) ðŸ‘‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1oN8BtqD8pK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
