{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The basics of RAG from scratch\n",
    "\n",
    "With this notebook you can ask questions about your own document. \n",
    "It uses ollama to run LLM's locally. \n",
    "\n",
    "Make sure you have downloaded and installed ollama from www.ollama.com.\n",
    "\n",
    "#### Contents\n",
    "0. Install and import packages\n",
    "1. Check available models in ollama + set system prompt\n",
    "2. Get the text document and split into paragraphs (chunks of text)\n",
    "3. Embeddings¶\n",
    "4. Set the prompt, create prompt embeddings and do similarity search\n",
    "5. Get response from the LLM\n",
    "\n",
    "#### Source\n",
    "source & acknowledgements: https://decoder.sh/videos/rag-from-the-ground-up-with-python-and-ollama "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows the flow in this flowdiagram. Please study it carefully.\n",
    "![slide1](slide_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in c:\\users\\jonathan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in c:\\users\\jonathan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ollama) (0.27.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\jonathan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.4.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\jonathan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jonathan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\jonathan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\jonathan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\jonathan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement json (from versions: none)\n",
      "ERROR: No matching distribution found for json\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama\n",
    "%pip install json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "import ollama\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check availabe models in ollama + set system prompt\n",
    "\n",
    "For this notebook you'll need two models:\n",
    "- an embedding model: nomic-embed-text\n",
    "- any LLM: tinyllama, mistral or other (choose yourself, set later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                   \tID          \tSIZE  \tMODIFIED       \n",
      "nomic-embed-text:latest\t0a109f422b47\t274 MB\t30 seconds ago\t\n",
      "mistral:latest         \t2ae6f6dd7a3d\t4.1 GB\t8 minutes ago \t\n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ollama' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#uncomment if necessary\n",
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#set the system prompt\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful reading assistant who answers questions \n",
    "        based on snippets of text provided in context. Answer only using the context provided, \n",
    "        being as concise as possible. If you're unsure, just say that you don't know.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get the text document and split into paragraphs (chunks of text)\n",
    "\n",
    "In this case, we simply use .txt files from the Project Gutenberg website.\n",
    "www.gutenberg.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# function to open a file and return paragraphs\n",
    "def parse_file(filename):\n",
    "    with open(filename, encoding=\"utf-8-sig\") as f:\n",
    "        paragraphs = []\n",
    "        buffer = []\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                buffer.append(line)\n",
    "            elif len(buffer):\n",
    "                paragraphs.append((\" \").join(buffer))\n",
    "                buffer = []\n",
    "        if len(buffer):\n",
    "            paragraphs.append((\" \").join(buffer))\n",
    "        return paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"lightblue\">\n",
    "\n",
    "1. Bestand openen: De functie opent een bestand met een opgegeven bestandsnaam, gebruikmakend van UTF-8-SIG encoding.\n",
    "2. Initialisatie: Twee variabelen worden aangemaakt: paragraphs (een lijst van paragrafen) en buffer (een tijdelijke opslag voor regels van een paragraaf).\n",
    "3. Regels lezen: Voor elke regel in het bestand:\n",
    "    - De regel wordt gestript van witruimte.\n",
    "    - Als de regel niet leeg is, wordt deze toegevoegd aan buffer.\n",
    "    - Als de regel leeg is en buffer bevat regels, worden de regels in buffer samengevoegd tot een paragraaf en toegevoegd aan paragraphs, waarna buffer wordt geleegd.\n",
    "\n",
    "4. Buffer controleren: Na het lezen van alle regels, als buffer nog regels bevat, worden deze samengevoegd en toegevoegd aan paragraphs.\n",
    "5. Resultaat: De functie retourneert de lijst van paragrafen.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Project Gutenberg eBook of Peter Pan', 'This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this ebook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook.', 'Title: Peter Pan']\n",
      "Total number of paragraphs: 1736\n"
     ]
    }
   ],
   "source": [
    "# open file as provided in the GitHub repo\n",
    "filename = \"peter-pan.txt\"\n",
    "paragraphs = parse_file(filename)\n",
    "print(paragraphs[0:3]) #print first 3 paragraphs\n",
    "print(f'Total number of paragraphs: {len(paragraphs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# functions to save, load and get the embeddings\n",
    "def save_embeddings(filename, embeddings):\n",
    "    # create dir if it doesn't exist\n",
    "    if not os.path.exists(\"embeddings\"):\n",
    "        os.makedirs(\"embeddings\")\n",
    "    # dump embeddings to json\n",
    "    with open(f\"embeddings/{filename}.json\", \"w\") as f:\n",
    "        json.dump(embeddings, f)\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    # check if file exists\n",
    "    if not os.path.exists(f\"embeddings/{filename}.json\"):\n",
    "        return False\n",
    "    # load embeddings from json\n",
    "    with open(f\"embeddings/{filename}.json\", \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_embeddings(filename, modelname, chunks):\n",
    "    # check if embeddings are already saved\n",
    "    if (embeddings := load_embeddings(filename)) is not False:\n",
    "        return embeddings\n",
    "    # get embeddings from ollama\n",
    "    embeddings = [\n",
    "        ollama.embeddings(model=modelname, prompt=chunk)[\"embedding\"]\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "    # save embeddings\n",
    "    save_embeddings(filename, embeddings)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"lightblue\">\n",
    "\n",
    "#### save_embeddings\n",
    "\n",
    "Slaat embeddinggegevens op in een JSON-bestand.\n",
    "\n",
    "- Werking:\n",
    "    - Controleert of de directory \"embeddings\" bestaat, en maakt deze indien nodig aan.\n",
    "    - Schrijft de embeddings naar een JSON-bestand met de opgegeven bestandsnaam binnen de \"embeddings\" directory.\n",
    "\n",
    "\n",
    "#### load_embeddings\n",
    "\n",
    "Laadt embeddinggegevens van een JSON-bestand als het bestaat.\n",
    "\n",
    "- Werking:\n",
    "\n",
    "    - Controleert of het JSON-bestand met de embeddings bestaat.\n",
    "    - Als het bestand bestaat, leest het de embeddings vanuit het JSON-bestand en retourneert deze.\n",
    "    - Als het bestand niet bestaat, retourneert het False.\n",
    "\n",
    "#### get_embeddings\n",
    "\n",
    "Verkrijgt embeddings voor gegeven tekststukken (chunks), hetzij door ze te laden van een opgeslagen bestand, hetzij door ze te genereren en vervolgens op te slaan.\n",
    "\n",
    "- Werking:\n",
    "    - Controleert of de embeddings al zijn opgeslagen door load_embeddings aan te roepen. Als ze bestaan, worden deze teruggegeven.\n",
    "    - Als de embeddings nog niet zijn opgeslagen, worden deze gegenereerd door de ollama.embeddings functie aan te roepen voor elk tekststuk (chunk).\n",
    "    - De nieuw gegenereerde embeddings worden opgeslagen met behulp van save_embeddings.\n",
    "    - De embeddings worden geretourneerd.\n",
    "\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1736"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the embeddings\n",
    "embeddings = get_embeddings(filename, \"nomic-embed-text\", paragraphs)\n",
    "len(embeddings) #should be same number as paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "metadata": {}
   },
   "source": [
    "## 4. Set the prompt, create prompt embeddings, do similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#set the prompt\n",
    "prompt = \"Tell me about tinke bell?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Create the prompt embeddings. Use the same embedding model!\n",
    "prompt_embedding = ollama.embeddings(model=\"nomic-embed-text\", prompt=prompt)[\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# find cosine similarity of every chunk to a given embedding\n",
    "def find_most_similar(needle, haystack):\n",
    "    needle_norm = norm(needle)\n",
    "    similarity_scores = [\n",
    "        np.dot(needle, item) / (needle_norm * norm(item)) for item in haystack\n",
    "    ]\n",
    "    return sorted(zip(similarity_scores, range(len(haystack))), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"lightblue\">\n",
    "\n",
    "#### find_most_similar\n",
    "\n",
    "Vindt de meest vergelijkbare embedding(s) in een lijst van embeddings (haystack) ten opzichte van een gegeven embedding (needle) door gebruik te maken van de cosinus-similariteit.\n",
    "\n",
    "- Werking:\n",
    "\n",
    "1. Normalisatie van de needle embedding: Bereken de norm (lengte) van de needle embedding.\n",
    "2. Berekening van similariteitsscores: Voor elke embedding in de haystack wordt de cosinus-similariteit met de needle berekend:\n",
    "    - De cosinus-similariteit tussen twee vectoren wordt berekend als de dot-product gedeeld door het product van hun normen.\n",
    "3. Sorteren van scores: De similariteitsscores worden samen met hun indexen gesorteerd in aflopende volgorde van similariteit.\n",
    "4. Resultaat: De gesorteerde lijst van tuples (similariteitsscore, index) wordt geretourneerd.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.6675811809011156, 1576),\n",
       " (0.6637123102530766, 248),\n",
       " (0.6530704549422695, 252),\n",
       " (0.6492243466576073, 1024),\n",
       " (0.6416622182936448, 278)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find paragraphs most similar to the prompt\n",
    "most_similar_chunks = find_most_similar(prompt_embedding, embeddings)[:5]\n",
    "most_similar_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Get a response from the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#set your model here!\n",
    "model='mistral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " Tinker Bell is a fairy who mends pots and kettles. She can become insolent and uses harsh language, as shown by her response \"You silly ass.\" This interaction suggests she has some level of intelligence and strong feelings.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "        model= model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPT\n",
    "                + \"\\n\".join(paragraphs[item[1]] for item in most_similar_chunks),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "print(\"\\n\\n\")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Own data set (recepten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Almond Meal Chicken Fingers', 'Ingredients: 2 Large chicken breasts or tenders 1 cup almond meal or almond flour 2 eggs Various spices to tase: Oregano, salt, pepper, cajun spice, cayenne pepper, Paprika, garlic salt, onion salt, thyme, etc Veggie as a side', 'In one mixing bowl add: 2 eggs whipped (for dip and roll) In another bowl: spices and 1 cup almond flour']\n",
      "Total number of paragraphs: 42\n"
     ]
    }
   ],
   "source": [
    "# open file as provided in the GitHub repo\n",
    "filename = \"recepten.txt\"\n",
    "paragraphs = parse_file(filename)\n",
    "print(paragraphs[0:3]) #print first 3 paragraphs\n",
    "print(f'Total number of paragraphs: {len(paragraphs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the embeddings for new dataset\n",
    "filename = \"recepten.txt\"\n",
    "embeddings = get_embeddings(filename, \"nomic-embed-text\", paragraphs)\n",
    "len(embeddings) #should be same number as paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " To make a Creamy Grape Salad, follow these steps:\n",
      "1. In a large bowl, beat the softened cream cheese, sour cream, sugar, and vanilla until blended.\n",
      "2. Add seedless red and green grapes to the mixture and toss to coat.\n",
      "3. Transfer the mixture to a serving bowl, cover, and refrigerate until serving.\n",
      "4. Just before serving, sprinkle with brown sugar and chopped pecans.\n",
      "No need for the ingredients listed under \"Optional\" in this recipe.\n"
     ]
    }
   ],
   "source": [
    "#set the prompt\n",
    "prompt = \"How do I make a creamy grape salad?\"\n",
    "\n",
    "# Create the prompt embeddings. Use the same embedding model!\n",
    "prompt_embedding = ollama.embeddings(model=\"nomic-embed-text\", prompt=prompt)[\"embedding\"]\n",
    "# find paragraphs most similar to the prompt\n",
    "most_similar_chunks = find_most_similar(prompt_embedding, embeddings)[:5]\n",
    "\n",
    "response = ollama.chat(\n",
    "        model= model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPT\n",
    "                + \"\\n\".join(paragraphs[item[1]] for item in most_similar_chunks),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "print(\"\\n\\n\")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"lightblue\">\n",
    "\n",
    "Hier kan ze zien dat dit RAG model het recept uit de recepten text file haalt. Dit recept is dus gebasseerd op het daadwerkelijke recept in de text file.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " To make Banana Nut Muffins, follow these steps:\n",
      "\n",
      "1. Preheat your oven to 350°F (175°C). Grease or line a muffin tin with paper liners.\n",
      "2. In a medium-sized bowl, whisk together the flour, baking soda, baking powder, and salt.\n",
      "3. In a separate large bowl, combine the mashed bananas, sugar, beaten egg, melted butter, and vanilla extract. Mix until well combined.\n",
      "4. Gradually add the dry ingredient mixture to the banana mixture. Stir until just combined, avoiding overmixing. Fold in the chopped nuts.\n",
      "5. Spoon the batter into the prepared muffin tin, filling each cup about two-thirds full.\n",
      "6. Bake for 20-25 minutes, or until a toothpick inserted into the center of a muffin comes out clean.\n",
      "7. Allow the muffins to cool in the tin for 5 minutes, then transfer to a wire rack to cool completely.\n",
      "8. Enjoy your delicious homemade Banana Nut Muffins!\n"
     ]
    }
   ],
   "source": [
    "#set the prompt\n",
    "prompt = \"How do I make a banana muffins?\"\n",
    "\n",
    "# Create the prompt embeddings. Use the same embedding model!\n",
    "prompt_embedding = ollama.embeddings(model=\"nomic-embed-text\", prompt=prompt)[\"embedding\"]\n",
    "# find paragraphs most similar to the prompt\n",
    "most_similar_chunks = find_most_similar(prompt_embedding, embeddings)[:5]\n",
    "\n",
    "response = ollama.chat(\n",
    "        model= model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPT\n",
    "                + \"\\n\".join(paragraphs[item[1]] for item in most_similar_chunks),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "print(\"\\n\\n\")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"lightblue\">\n",
    "\n",
    "Ook hier basseerd hij het recept op recepten uit de text file.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " To make spaghetti, follow steps 4-5 in the provided recipe for Spaghetti Bolognese. Here's a summary:\n",
      "\n",
      "1. Cook the spaghetti noodles according to the package instructions.\n",
      "2. Drain and set aside.\n",
      "3. Serve the cooked spaghetti topped with the prepared Bolognese sauce (as described in steps 1-6 of the recipe) and sprinkled with Parmesan cheese.\n",
      "\n",
      "Enjoy your homemade spaghetti!\n"
     ]
    }
   ],
   "source": [
    "#set the prompt\n",
    "prompt = \"How do I make spaghetti?\"\n",
    "\n",
    "# Create the prompt embeddings. Use the same embedding model!\n",
    "prompt_embedding = ollama.embeddings(model=\"nomic-embed-text\", prompt=prompt)[\"embedding\"]\n",
    "# find paragraphs most similar to the prompt\n",
    "most_similar_chunks = find_most_similar(prompt_embedding, embeddings)[:5]\n",
    "\n",
    "response = ollama.chat(\n",
    "        model= model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPT\n",
    "                + \"\\n\".join(paragraphs[item[1]] for item in most_similar_chunks),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "print(\"\\n\\n\")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"lightblue\">\n",
    "\n",
    "Nu stel ik een iets bredere/vagere vraag, alleen spaghetti. Hier krijg ik dus een niet compleet antwoord. Ik krijg alleen het laatste deel van het recept mee in mijn antwoord. \n",
    "\n",
    "Dit kan misschien ook liggen aan iets anders, en niet aan de formulering van mijn vraag.\n",
    "\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
