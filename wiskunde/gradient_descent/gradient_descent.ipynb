{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wat is gradient descent\n",
    "\n",
    "Gradient Descent is een optimalisatie-algoritme dat wordt gebruikt in machine learning en deep learning om de waarden van parameters (coeffici√´nten) van een functie (f) te minimaliseren die de kostenfunctie (fout) minimaliseert.\n",
    "\n",
    "Het werkt door iteratief de helling van de kostenfunctie te berekenen met betrekking tot de parameters en de parameters dienovereenkomstig aan te passen om de kosten te minimaliseren.\n",
    "\n",
    "Hier is een eenvoudige uitleg van de stappen:\n",
    "\n",
    "1. Initialiseer willekeurige waarden voor de parameters.\n",
    "2. Bereken de afgeleiden van de functie\n",
    "3. Bepaal de learning rate\n",
    "4. Update de parameterwaarden door de huidige waarden te verminderen met de berekende gradient vermenigvuldigd met een leerfactor (ook wel learning rate genoemd).\n",
    "5. Herhaal de stappen 2 tot 4 tot de kosten niet meer significant verminderen of na een vastgesteld aantal iteraties.\n",
    "Het doel is om de set parameters te vinden die de kosten minimaliseren en dus de beste voorspellingen geven.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waarom gradient descent\n",
    "\n",
    "Gradient Descent is een fundamenteel onderdeel van veel machine learning algoritmen om verschillende redenen:\n",
    "\n",
    "1. **Effici√´ntie**: Gradient Descent kan effici√´nter zijn dan andere optimalisatie methoden, vooral bij het werken met grote datasets. Het is een iteratief algoritme, wat betekent dat het kan worden gestopt wanneer de verbetering onder een bepaalde drempel valt, waardoor onnodige berekeningen worden voorkomen.\n",
    "\n",
    "2. **Schaalbaarheid**: Het werkt goed met datasets van elke grootte. Dit is vooral belangrijk in het tijdperk van Big Data, waar datasets vaak enorm groot zijn.\n",
    "\n",
    "3. **Optimalisatie**: Het helpt bij het vinden van de optimale waarden van de parameters van een model, wat leidt tot de laagst mogelijke kosten (fout). Dit is cruciaal voor het trainen van effectieve machine learning modellen.\n",
    "\n",
    "4. **Generalisatie**: Het kan worden toegepast op een breed scala aan machine learning modellen, waaronder lineaire regressie, logistieke regressie, neurale netwerken, enz.\n",
    "\n",
    "5. **Aanpasbaarheid**: Het kan worden aangepast en verbeterd door verschillende varianten zoals Stochastic Gradient Descent (SGD), Mini-batch Gradient Descent, en adaptieve methoden zoals Adam, die verschillende leerfrequenties voor verschillende parameters gebruiken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent in python\n",
    "\n",
    "Gradient descent is een optimalisatie-algoritme dat wordt gebruikt om de minimale kostenfunctie te vinden. We passen het toe op de functie J(Œ∏)= ùúÉ^2. Hier zijn de stappen:\n",
    "\n",
    "1. Initialiseer willekeurige waarden voor de parameters. We stellen een willekeurige waarde in voor Œ∏.\n",
    "\n",
    "    ```python\n",
    "    theta = np.random.randn()\n",
    "    ```\n",
    "\n",
    "2. Bereken de afgeleiden van de functie. De afgeleide van onze functie J(Œ∏)= ùúÉ^2 is 2Œ∏.\n",
    "\n",
    "    ```python\n",
    "    gradient = 2 * theta\n",
    "    ```\n",
    "\n",
    "3. Bepaal de learning rate. Dit is een hyperparameter die bepaalt hoe snel we naar de minimale kostenfunctie bewegen. Een te hoge learning rate kan leiden tot het overslaan van de minimale kostenfunctie, terwijl een te lage learning rate kan leiden tot een langzaam convergentieproces.\n",
    "\n",
    "    ```python\n",
    "    learning_rate = 0.01\n",
    "    ```\n",
    "\n",
    "4. Update de parameterwaarden door de huidige waarden te verminderen met de berekende gradient vermenigvuldigd met een leerfactor (ook wel learning rate genoemd). Dit wordt gedaan met de formule:\n",
    "\n",
    "    Œ∏ = Œ∏ - Œ± * ‚àáJ(Œ∏)\n",
    "\n",
    "    waarbij Œ± de learning rate is en ‚àáJ(Œ∏) de gradient is.\n",
    "\n",
    "    ```python\n",
    "    theta = theta - learning_rate * gradient\n",
    "    ```\n",
    "\n",
    "5. Herhaal de stappen 2 tot 4 tot de kosten niet meer significant verminderen of na een vastgesteld aantal iteraties. In dit geval herhalen we het proces voor 1000 iteraties.\n",
    "\n",
    "    ```python\n",
    "    for i in range(1000):\n",
    "        gradient = 2 * theta\n",
    "        theta = theta - learning_rate * gradient\n",
    "    ```\n",
    "\n",
    "6. Aan het einde van de loop hebben we de waarde van Œ∏ die de kosten minimaliseert.\n",
    "\n",
    "    ```python\n",
    "    print(\"De waarde van theta die de kosten minimaliseert is: \", theta)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De waarde van theta die de kosten minimaliseert is:  1.51112775399588e-09\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Initialiseer willekeurige waarden voor de parameters.\n",
    "theta = np.random.randn()\n",
    "\n",
    "# 3. Bepaal de learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 5. Herhaal de stappen 2 tot 4 tot de kosten niet meer significant verminderen of na een vastgesteld aantal iteraties.\n",
    "for i in range(1000):\n",
    "    # 2. Bereken de afgeleiden van de functie\n",
    "    gradient = 2 * theta\n",
    "\n",
    "    # 4. Update de parameterwaarden door de huidige waarden te verminderen met de berekende gradient vermenigvuldigd met een leerfactor (ook wel learning rate genoemd).\n",
    "    theta = theta - learning_rate * gradient\n",
    "\n",
    "print(\"De waarde van theta die de kosten minimaliseert is: \", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent op MSE\n",
    "\n",
    "De Mean Squared Error (MSE) kostenfunctie is een veelgebruikte kostenfunctie voor regressieproblemen. Het wordt gedefinieerd als het gemiddelde van de kwadraten van de verschillen tussen de werkelijke en de voorspelde waarden.\n",
    "\n",
    "De MSE kostenfunctie is gedefinieerd als:\n",
    "\n",
    "$$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "waarbij:\n",
    "\n",
    "- $n$ is het aantal observaties,\n",
    "- $y_i$ is de werkelijke waarde van de i-de observatie,\n",
    "- $\\hat{y}_i$ is de voorspelde waarde van de i-de observatie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dit is de afgeleiden van de MSE kosten functie waarmee ik verder ga.\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^{n} -2(y_i - (mx_i + b)) $$\n",
    "\n",
    "1. Initialiseer willekeurige waarden voor de parameters $m$ en $b$.\n",
    "\n",
    "2. Bereken de parti√´le afgeleiden van de MSE-functie met betrekking tot $m$ en $b$. Deze afgeleiden geven de richting aan waarin de kosten het snelst veranderen.\n",
    "\n",
    "3. Bepaal de learning rate (leerfactor), die aangeeft hoe groot de stap moet zijn die we nemen in de richting van de negatieve gradi√´nt om de kosten te verlagen. De learning rate is meestal een kleine waarde, zoals Œ±=0.01 \n",
    "\n",
    "4. Herhaal de stappen 2 en 3 tot de kosten niet meer significant verminderen of na een vastgesteld aantal iteraties. Dit betekent dat we doorgaan met het aanpassen van de parameters totdat we een minimaal punt bereiken waar de kosten niet langer significant dalen.\n",
    "\n",
    "    Door deze stappen herhaaldelijk uit te voeren, zullen de parameterwaarden convergeren naar een set waarden die de kosten minimaliseren en dus de beste voorspellingen mogelijk maken. Dit is het doel van gradient descent in het optimaliseren van de parameters van een model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "![Design](NN_Design.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
