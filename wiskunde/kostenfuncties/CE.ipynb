{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy is een andere populaire kostenfunctie in machine learning, vooral bij classificatieproblemen. Het meet de prestaties van een classificatiemodel waarvan de uitvoer een waarschijnlijkheidswaarde tussen 0 en 1 is. Cross-entropy neemt toe naarmate de voorspelde waarschijnlijkheid afwijkt van de werkelijke waarde. De formule voor cross-entropy is:\n",
    "\n",
    "Cross-Entropy $ = -\\sum(y * log(p) + (1 - y) * log(1 - p))$\n",
    "\n",
    "Waarbij:\n",
    "\n",
    "- $y$ is de werkelijke waarde\n",
    "- $p$ is de voorspelde waarschijnlijkheid\n",
    "\n",
    "Cross-entropy is belangrijk in AI en machine learning omdat het een effectieve manier biedt om de prestaties van een classificatiemodel te meten. Het is vooral nuttig bij training van logistieke regressie en neurale netwerken. Het minimaliseren van de cross-entropy is een manier om de nauwkeurigheid van het model te verbeteren. Het is vooral nuttig wanneer de uitvoer van een model de waarschijnlijkheid van bepaalde uitkomsten moet weergeven."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
